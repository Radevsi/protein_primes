{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Notebooks for Thesis.\n",
    "\n",
    "Supports KMeans and DBSCAN. Also performs preliminary tests for optimal algorithm parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Tuple, Union \n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# import math\n",
    "# from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import py3Dmol\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from moleculib.protein.datum import ProteinDatum\n",
    "from moleculib.graphics.py3Dmol import plot_py3dmol, plot_py3dmol_grid\n",
    "\n",
    "from moleculib.protein.transform import (\n",
    "    ProteinCrop,\n",
    "    TokenizeSequenceBoundaries,\n",
    "    ProteinPad,\n",
    "    MaybeMirror,\n",
    "    BackboneOnly,\n",
    "    DescribeChemistry\n",
    ")\n",
    "\n",
    "from helpers.database import populate_representations, get_column, get_scalars, whatis\n",
    "from helpers.edges import connect_edges, CascadingEdges\n",
    "from helpers.cascades import Cascade, MakeCascade, Metrics, MetricsPair, MakeMetricsPair\n",
    "from helpers.load_data import LoadData, save_df, save_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"data/denim-energy-1008-embeddings\"\n",
    "embeddings_file = \"encoded_dataset.pkl\"\n",
    "sliced_proteins_file = \"sliced_dataset.pkl\"\n",
    "tsne_file = \"encoded_dataset_tsne.json\"\n",
    "\n",
    "# Load data from files\n",
    "dataloader = LoadData(FOLDER, embeddings_file, sliced_proteins_file, tsne_file)\n",
    "dataloader.load_all()\n",
    "\n",
    "# Make objects\n",
    "reps, _ = populate_representations(dataloader.encoded_dataset, \n",
    "                                   dataloader.sliced_dataset, \n",
    "                                   dataloader.tsne_dataset)\n",
    "df = reps.to_dataframe()\n",
    "print(f\"Loaded full dataset: {df.shape}\")\n",
    "\n",
    "# Calculate the number of nodes per level\n",
    "nodes_per_level = df.groupby('level').size()\n",
    "print(\"Number of nodes per level:\")\n",
    "display(nodes_per_level)\n",
    "\n",
    "# Count the \"None\" datums\n",
    "n_none_datums = df[df['datum'].isnull()].shape[0]\n",
    "print(f\"Number of None datums: {n_none_datums}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do data cleaning as follows: Drop the none datums. Then every key in the edges bottom up dictionary that does not cascade to the top, is dropped from the dataframe (including that entire pdb id). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process for clustering algos...\n",
    "\n",
    "df_sample = df.dropna(subset=['datum']).reset_index(drop=True)  # drop nans\n",
    "df_sample = df_sample[df_sample['level'] != 0].reset_index(drop=True)  # drop level 0\n",
    "\n",
    "print(f\"Shape of sample after drops: {df_sample.shape}\")\n",
    "\n",
    "# Compute edges\n",
    "kernel, stride = 5, 2\n",
    "edges_top_down, edges_bottom_up, n_misses = connect_edges(df_sample, kernel, stride)\n",
    "make_cascades = CascadingEdges(edges_bottom_up)\n",
    "print(f\"Misses: {n_misses}\")\n",
    "whatis(edges_top_down, edges_bottom_up, make_cascades)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "if SAVE:\n",
    "\n",
    "    ABS_PATH_TO_VIZ = \"/Users/moniradev/Documents/projects/foreign/picture-picture/public/data\"\n",
    "\n",
    "    # Drop scalars and datums for visualization\n",
    "    save_df(df_sample.drop(columns=['scalar_rep', 'datum']), \"df_final\", folder=ABS_PATH_TO_VIZ)\n",
    "    save_edges(edges_bottom_up, \"edges_bottom_up_final\", folder=ABS_PATH_TO_VIZ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe based on both pdb_id and level\n",
    "# 1dabA\n",
    "filtered_df = df_sample[(df_sample['pdb_id'] == '1dbvnA') & (df_sample['level'] == 2)]\n",
    "# display(filtered_df)\n",
    "print(filtered_df.index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.cascades import MakeMetricsPair\n",
    "# u, v = 176533, 200852\n",
    "\n",
    "# u, v = 188260, 194176\n",
    "\n",
    "u, v = 203599, 180218\n",
    "\n",
    "\n",
    "# u, v = 187047, 150546\n",
    "\n",
    "# u, v = 150633, 187126\n",
    "\n",
    "# u, v = 48430, 48431\n",
    "\n",
    "# u, v = 204856, 197267\n",
    "\n",
    "# u, v = 202778, 197051\n",
    "\n",
    "# u, v = 201935, 1109   \n",
    "\n",
    "# ubi = \"MQIFVKTLTG KTITLEVEPS DTIENVKAKI QDKEGIPPDQ QRLIFAGKQL EDGRTLSDYN IQKESTLHLV LRLRGG\"\n",
    "ubiquitin_scaffold = \"MQIFVKTLTGKTITLEVEPSDTIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG\"\n",
    "# MQIFVKTLT-[Motif]-GKTITLEVEPSDTIENVKAKIQDKEGIPPDQQRLIFAGKQLEDGRTLSDYNIQKESTLHLVLRLRGG\n",
    "\n",
    "def scaffolded_motif(motif, scaffold=ubiquitin_scaffold):\n",
    "    return f\"{scaffold[:9]}{motif}{scaffold[9:]}\"\n",
    "\n",
    "\n",
    "# Position 8 and 9\n",
    "\n",
    "\n",
    "us, vs = make_cascades(u), make_cascades(v)\n",
    "print(us, vs)\n",
    "metrics_pair = MakeMetricsPair(df_sample, us, vs)()\n",
    "# cas1, cas2 = metrics_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import d\n",
    "import random\n",
    "\n",
    "def generate_random_aa_sequence(length=13):\n",
    "    letters = list(filter(lambda x: x not in ['UNK', 'MASK', 'PAD'], d.values()))\n",
    "    return ''.join(random.choices(letters, k=length))\n",
    "\n",
    "random_sequence = generate_random_aa_sequence()\n",
    "print(\"Random 13-letter amino acid sequence:\", random_sequence)\n",
    "print(\"Random scaffold using this raandom motif:\", scaffolded_motif(random_sequence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view1, view2, indices1, indices2 = metrics_pair.plot_cascade_pair(\n",
    "    metrics_pair.cascade1, metrics_pair.cascade2, return_indices=True\n",
    ")\n",
    "print(f\"Indices1: {indices1}\")\n",
    "print(f\"Indices2: {indices2}\")\n",
    "view1.show()\n",
    "view2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SavePDB:\n",
    "    \"\"\"Given a dataframe of encodings, save .pdb files of datums specified by index.\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "\n",
    "    def __call__(self, u: int, filename: str = \"\", folder: str = \"saved_pdbs/\"):\n",
    "        \"\"\"Given an index, or list of indices in the dataframe, save the corresponding PDB data as a .pdb file.\n",
    "            The naming scheme is `index.pdb` where index is the index of the datum in the dataframe.\n",
    "        \"\"\"\n",
    "        if isinstance(u, list):\n",
    "            for index in u:\n",
    "                pdb_data = self.df.iloc[index]['datum'].to_pdb_str()\n",
    "                self.save_as_pdb(pdb_data, f\"{filename+str(u)}\", folder)\n",
    "        else:\n",
    "            pdb_data = self.df.iloc[u]['datum'].to_pdb_str()\n",
    "            self.save_as_pdb(pdb_data, f\"{filename+str(u)}\", folder)\n",
    "\n",
    "    @staticmethod\n",
    "    def save_as_pdb(pdb_data: str, filename: str, folder: str = \"saved_pdbs/\"):\n",
    "        with open(f\"{folder}/{filename}.pdb\", \"w\") as file:\n",
    "            file.write(pdb_data)\n",
    "\n",
    "saved_pdb = SavePDB(df_sample)\n",
    "saved_pdb(u)\n",
    "\n",
    "df_sample.iloc[[u,v]]['datum'].values[0].to_pdb_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Results for ids: {u}, {v}\")\n",
    "print(metrics_pair)\n",
    "print(\"First column sequences (short): \")\n",
    "short_seq1 = aa_map(metrics_pair.cascade1.sequences[0])[0]\n",
    "short_seq2 = aa_map(metrics_pair.cascade2.sequences[0])[0]\n",
    "print(short_seq1)\n",
    "print(short_seq2)\n",
    "print(\"Scaffolded motifs:\")\n",
    "print(scaffolded_motif(short_seq1))\n",
    "print(scaffolded_motif(short_seq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtools import tm_align\n",
    "\n",
    "coords1 = metrics_pair.cascade1.datums[0].atom_coord[:, 1, :].reshape(13, 3)\n",
    "coords2 = metrics_pair.cascade2.datums[0].atom_coord[:, 1, :].reshape(13, 3)\n",
    "seq1 = aa_map(metrics_pair.cascade1.sequences[0])[0]\n",
    "seq2 = aa_map(metrics_pair.cascade2.sequences[0])[0]\n",
    "whatis(coords1, coords2, seq1, seq2)\n",
    "\n",
    "print()\n",
    "print(seq1)\n",
    "print(seq2)\n",
    "res = tm_align(coords1, coords2, 'A'*13, 'A'*13)\n",
    "print(res.t, res.u)\n",
    "\n",
    "print(res.tm_norm_chain1)\n",
    "print(res.tm_norm_chain2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd20s = ['6PE9', '6TKB', '6PE8', '6TKF', '6TKE', '6TKD', '6TKC', '1QSC', '6BRB', '3LKJ',\n",
    "         '6PE7', '1ALY']\n",
    "ms_related = ['6H24', '1PY9', '5HIU', '6FG1', '6FG2', '4Q6R', '4GMV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from helpers.utils import CheckPDBs\n",
    "\n",
    "checker = CheckPDBs(cd20s + ms_related)\n",
    "# common_keys, missed_keys = checker.ds(chroma_full.encoded_dataset)\n",
    "common_keys, missed_keys = checker.df(df_sample)\n",
    "\n",
    "# Print the PDB IDs not in common\n",
    "# print(f\"Length of request pdb ids: {len(pdbids)}, length of encoded keys: {len(chroma_full.encoded_dataset.keys())}\")\n",
    "print(f\"There are {len(missed_keys)} missed keys and {len(common_keys)} common keys\")\n",
    "print()\n",
    "print(\"Missed pdb ids:\")\n",
    "print(missed_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to get rows where the 'pdb_id' contains '1f00'\n",
    "filtered_df = df_sample[df_sample['pdb_id'].str.contains('1bhx')]\n",
    "print(filtered_df['pdb_id'].unique())\n",
    "df_sample['pdb_id'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moleculib.assembly.datum import AssemblyDatum\n",
    "\n",
    "# Given a list of PDB ids, pull them from moleculib and visualize\n",
    "\n",
    "max_chain_len = 253  # max length for denim-energy model\n",
    "protein_transform = [\n",
    "    ProteinCrop(crop_size=max_chain_len),\n",
    "    TokenizeSequenceBoundaries(),\n",
    "    MaybeMirror(hand='left'),\n",
    "    ProteinPad(pad_size=max_chain_len, random_position=False),\n",
    "    BackboneOnly(filter=True),\n",
    "    DescribeChemistry(),\n",
    "]\n",
    "\n",
    "def transform(datum):\n",
    "    return reduce(lambda x, f: f.transform(x), protein_transform, datum)\n",
    "\n",
    "class FetchPDBids:\n",
    "    \"\"\"Fetch PDB ids as AssemblyDatums.\"\"\"\n",
    "    def __init__(self, pdb_ids: List[str]):\n",
    "        self.pdb_ids = [pdb_id.lower() for pdb_id in pdb_ids]\n",
    "        self.datums = []\n",
    "        self.transformed = []  # list of transformed ProteinDatums\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Fetching {len(self.pdb_ids)} PDB IDs...\", end=\" \")\n",
    "        for pdb_id in self.pdb_ids:\n",
    "            assembly = AssemblyDatum.fetch_pdb_id(pdb_id,)\n",
    "            for datum in assembly.protein_data:\n",
    "                # datum.idcode = pdb_id\n",
    "                self.datums.append(datum)\n",
    "            print(f\"{pdb_id}, \", end=\"\")\n",
    "        print(\"\\nDone\")\n",
    "\n",
    "    def togrid(self, k=None, num_columns=3, use_transformed=False):\n",
    "        if k is None:\n",
    "            k = len(self.datums)\n",
    "        if use_transformed:\n",
    "            if self.transformed == []:\n",
    "                self.transform()\n",
    "            datum_grid = self.make_grid(self.transformed[:k], num_columns)\n",
    "        else:\n",
    "            datum_grid = self.make_grid(self.datums[:k], num_columns)\n",
    "        return datum_grid\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_grid(datums: List[ProteinDatum], num_columns=3):\n",
    "        return [datums[i:i + num_columns] for i in range(0, len(datums), num_columns)]\n",
    "    \n",
    "    def transform(self):\n",
    "        self.transformed = [transform(datum) for datum in self.datums]\n",
    "\n",
    "fetcher = FetchPDBids(master_list_unique)\n",
    "fetcher()\n",
    "fetcher.transform()\n",
    "print(f\"Number of fetched datums: {len(fetcher.datums)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d.idcode for d in fetcher.transformed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list index of \"2KL8\" from missed_keys\n",
    "\n",
    "term_pair = ['2kl8', '2ln3']\n",
    "term_pair_datums = [fetcher.datums[fetcher.pdb_ids.index(pdb_id)] for pdb_id in term_pair]\n",
    "print(term_pair_datums)\n",
    "fetcher.make_grid(term_pair_datums, num_columns=2)\n",
    "plot_py3dmol_grid(fetcher.make_grid(term_pair_datums, num_columns=2),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(term_pair_datums[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_py3dmol_grid(fetcher.togrid(k=15, num_columns=3, use_transformed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmtools import tm_align\n",
    "from helpers.utils import DistanceMapMetric, DistanceSeqMetric\n",
    "\n",
    "\n",
    "def calphas_from_datum(datum):\n",
    "    return datum.atom_coord[:, 1, :].reshape(-1, 3)\n",
    "\n",
    "coords1 = calphas_from_datum(term_pair_datums[0])\n",
    "coords2 = calphas_from_datum(term_pair_datums[1])\n",
    "print(coords1.shape, coords2.shape)\n",
    "\n",
    "res = tm_align(coords1, coords2, 'A'*85, 'A'*83)\n",
    "print(res.t, res.u)\n",
    "\n",
    "print(res.tm_norm_chain1)\n",
    "print(res.tm_norm_chain2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DistanceMapMetric()(term_pair_datums[0], term_pair_datums[1]))\n",
    "DistanceSeqMetric()(term_pair_datums[0], term_pair_datums[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the list of fetched datums into a grid with at most 3 columns\n",
    "num_columns = 3\n",
    "datum_grid = fetcher.togrid(num_columns, use_transformed=True)\n",
    "print(datum_grid)\n",
    "\n",
    "plot_py3dmol_grid(datum_grid)\n",
    "# common_.plot(view=py3Dmol.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Idx2Datum:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __call__(self, *idxs):\n",
    "        return self.df.loc[idxs, 'datum'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.statistics import ComputeDistanceMatrix, RadiusNeighbors, sort_distance_graph\n",
    "\n",
    "distMx = ComputeDistanceMatrix(df_sample)\n",
    "\n",
    "def analyze_distance_matrix(level):\n",
    "    # Compute the distance matrix at the specified level\n",
    "    matrix = distMx(level=level)\n",
    "\n",
    "    # Convert the csr matrix to a dense format to handle it with numpy\n",
    "    dense_matrix = matrix.toarray()\n",
    "\n",
    "    # Flatten the matrix to get the distribution of distance values\n",
    "    flat_distances = dense_matrix.flatten()\n",
    "\n",
    "    # Calculate quantiles\n",
    "    quantiles = np.quantile(flat_distances, [0.25, 0.5, 0.75])\n",
    "\n",
    "    # Print quantile information\n",
    "    print(\"Quantiles of distance values:\")\n",
    "    print(f\"25th percentile: {quantiles[0]}\")\n",
    "    print(f\"50th percentile (median): {quantiles[1]}\")\n",
    "    print(f\"75th percentile: {quantiles[2]}\")\n",
    "\n",
    "    # Calculate and print distribution details\n",
    "    print(\"Distribution details of distance values:\")\n",
    "    print(f\"Minimum distance: {flat_distances.min()}\")\n",
    "    print(f\"Maximum distance: {flat_distances.max()}\")\n",
    "    print(f\"Mean distance: {flat_distances.mean()}\")\n",
    "    print(f\"Standard deviation: {flat_distances.std()}\")\n",
    "\n",
    "# Example usage\n",
    "analyze_distance_matrix(level=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import aa_map\n",
    "aa_map(MakeCascade.datum_to_sequence(metrics_pair.cascade1.datums[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class CascadeDropper:\n",
    "    def __init__(self, df, edges_bottom_up):\n",
    "        self.df = df\n",
    "        self.edges_bottom_up = edges_bottom_up\n",
    "\n",
    "    def drop_random_cascades(self):\n",
    "        # Iterate through the bottom indices\n",
    "        bottom_indices = self.df[self.df['level'] == 1].index\n",
    "        # bottom_indices = [idx for idx in self.edges_bottom_up if self.df.loc[idx, 'level'] == 1]\n",
    "\n",
    "        # Initialize list to collect cascades to drop\n",
    "        cascades_to_drop = []\n",
    "\n",
    "        # Get cascades and randomly drop 20% of them\n",
    "        for idx in bottom_indices:\n",
    "            if random.random() < 0.20:  # 20% chance to drop this cascade\n",
    "                cascade = make_cascades(idx, verbose=False)\n",
    "                cascades_to_drop.extend(cascade)\n",
    "\n",
    "        # Collect indices from the dropped cascades\n",
    "        indices_to_drop_from_cascades = list(set(cascades_to_drop))\n",
    "\n",
    "        # Drop these indices from the dataframe\n",
    "        self.df = self.df.drop(indices_to_drop_from_cascades)\n",
    "        print(f\"Shape of dataframe after dropping 20% of cascades: {self.df.shape}\")\n",
    "        return self.df\n",
    "\n",
    "# Instantiate and use the CascadeDropper\n",
    "cascade_dropper = CascadeDropper(df_sample, edges_bottom_up)\n",
    "df_sample_clean = cascade_dropper.drop_random_cascades()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic mutation is sequential, but is there a way in which biology conserves structure as well?\n",
    "\n",
    "10^16 13-mers. \n",
    "\n",
    "Is there a case where something is similar in structure space and dissimilar in sequence space? \n",
    "\n",
    "<ol>\n",
    "<li>Find pairs similar in ophiuchus latent space that map to divergent pairs in higher levels (in ophiuchus space)</li>\n",
    "<li>Of the 111k 5-mer vector space embeddings, what is the subset of those vectors which </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "The parts that are closer at the lower level and appear in things very different suggests of big mutation (or big difference). \n",
    "\n",
    "1. For a specific level: find dmin, then list out all the pairs that are 10 x dmin.\n",
    "\n",
    "\n",
    "The model provides an alignment mechanism in latent space (\"Multiple Latent Alignment\")\n",
    "\n",
    "\n",
    "Thesis is the idea of building blocks. Why are we so sure that biology has building blocks? The reason is that biology builds 300-mer proteins and it cnanot be done from scratch. \n",
    "\n",
    "A helix is an examples of a structural motif \n",
    "\n",
    "A helix can have lots  of amino acids, but it must present a correct seq of amino acids to build a helix helxi (ie building locko). There are specific locations that need to match to build a building block. The same exact sequecne is too over constarining. The right building block is the right number f amino acids in the right place. \n",
    "\n",
    "\n",
    "<i>Small number of amino acids in the right place, that interact with another set of blocks that are a small number of amino acids in the right place.</i>\n",
    "\n",
    "If we take things close in level 1 and swap them out using AlphaFold, what does that build? \n",
    "\n",
    "But then isn't a beta helix in some sense exactly this? \n",
    "\n",
    "Chapters:\n",
    "\n",
    "Chapter 1:\n",
    "\n",
    "level 1 pairs that are close, map to level 4 pairs that are broad. Call those \"building blocks\".  What is the intellectual insight of a building block? \n",
    "\n",
    "Chapter 2: \n",
    "\n",
    "<b>If I have time:</b> AlphaFold. Biology would have taken a part, mutated a point mutation, and ended up with the same structure. THen biology would have to do a crossover mutation and reuse it somewhere else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 202409\n",
    "# df_sample.iloc[202409]\n",
    "df[df['pdb_id'] == '1b2wL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.statistics import ComputeDistanceMatrix, RadiusNeighbors, sort_distance_graph\n",
    "\n",
    "distMx = ComputeDistanceMatrix(df_sample)\n",
    "matrix = distMx(level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick sample candidates\n",
    "main_df = df_small.dropna(subset=['datum'])\n",
    "\n",
    "u, v = 1342, 3834\n",
    "\n",
    "display(main_df.loc[[u, v]])\n",
    "us, vs = make_cascades(u), make_cascades(v)\n",
    "print(us, vs)\n",
    "\n",
    "\n",
    "\n",
    "MakeCascade.plot_datums(Idx2Datum(df_small)(us)).show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
