{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ophiuchus Encode\n",
    "\n",
    "Encode a custom set of pdbs through a pre-trained ophiuchus.\n",
    "\n",
    "This notebook should be used solely to generate embeddings and slice datums (and potentially tsne values) given a specific list of desired pdb ids. It applies selected transforms accordingly as well.\n",
    "\n",
    "Checkpoint around 4 o'clock on Saturday the 18th, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "# Fix up the path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "module_path1 = os.path.abspath(os.path.join('../../../ophiuchus'))\n",
    "module_path2 = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if module_path1 not in sys.path:\n",
    "    sys.path.append(module_path1)\n",
    "if module_path2 not in sys.path:\n",
    "    sys.path.append(module_path2)\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import reduce, partial\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import jax\n",
    "import haiku as hk\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from kheiron.pipeline.registry import Registry\n",
    "# from absl import app, flags\n",
    "\n",
    "from moleculib.protein.datum import ProteinDatum\n",
    "from moleculib.assembly.datum import AssemblyDatum\n",
    "from moleculib.protein.transform import (\n",
    "    ProteinCrop,\n",
    "    TokenizeSequenceBoundaries,\n",
    "    ProteinPad,\n",
    "    MaybeMirror,\n",
    "    BackboneOnly,\n",
    "    DescribeChemistry\n",
    ")\n",
    "\n",
    "from model.base.sequence_convolution import moving_window\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(f\"Saving embeddings to {path}\")\n",
    "# print(f\"Do TSNE: {FLAGS.tsne}\")\n",
    "# print(f\"Save HTML: {FLAGS.html}\")\n",
    "\n",
    "trained_model = \"denim-energy-1008\"\n",
    "\n",
    "registry = Registry('ophiuchus', base_path=os.getenv(\"ALLAN_REGISTRY\"))\n",
    "platform = registry.get_platform(trained_model, read_only=True)\n",
    "cfg = platform.cfg\n",
    "\n",
    "stride = cfg['trainer']['model']['autoencoder']['stride']\n",
    "kernel_size = cfg['trainer']['model']['autoencoder']['kernel_size']\n",
    "depth = len(cfg['trainer']['model']['autoencoder']['layers'])\n",
    "\n",
    "\n",
    "max_chain_len = 253  # crop size for denim energy\n",
    "chain_len = max_chain_len\n",
    "\n",
    "# we initialize the slices as [i:i+1] for each i in the chain    \n",
    "slices = np.stack([np.arange(0, chain_len), np.arange(1, chain_len + 1)],axis=-1)\n",
    "\n",
    "slices_per_level = {0: slices}\n",
    "# we then reduce as we would in convolutions, but we keep track of the sizes instead\n",
    "for i in range(depth - 1):\n",
    "    windows = moving_window(np.arange(slices.shape[0]), kernel_size, stride)\n",
    "    # breakpoint()\n",
    "    slices_ = slices[windows]\n",
    "    slices = np.stack([slices_[:, :, 0].min(axis=-1), slices_[:, :, 1].max(axis=-1)],axis=-1)\n",
    "    slices_per_level[i + 1] = slices\n",
    "\n",
    "protein_transform = [\n",
    "    ProteinCrop(crop_size=max_chain_len),\n",
    "    TokenizeSequenceBoundaries(),\n",
    "    MaybeMirror(hand='left'),\n",
    "    ProteinPad(pad_size=max_chain_len, random_position=False),\n",
    "    BackboneOnly(filter=True),\n",
    "    DescribeChemistry(),\n",
    "]\n",
    "\n",
    "def transform(datum):\n",
    "    return reduce(lambda x, f: f.transform(x), protein_transform, datum)\n",
    "\n",
    "rng_seq = hk.PRNGSequence(42)\n",
    "premodel = platform.instantiate_model()\n",
    "forward_ = hk.transform(lambda *a, **ka: premodel()(*a, **ka))\n",
    "\n",
    "@jax.jit\n",
    "def _autoencoder(params, rng, datum):\n",
    "    return forward_.apply(params, rng, datum)\n",
    "\n",
    "base_params = platform.get_params(-1)\n",
    "def autoencoder(batch):\n",
    "    return _autoencoder(base_params, next(rng_seq), batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dataset has 69 samples\n",
      "(69,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset\n",
    "\n",
    "# This list focuses on pre-defined pairs of interest, as well as beta helices\n",
    "beta_helix_and_friends = ['2jp7', '1prp', '3nxq', '1gca', '1pcl', '1xiq', '2pqe', '1kzq', '4mzu', '1wpc', '1fnu', '4g6r', '4jj2', '3hno', '1lxa', '6ria', '1hg9', '1dcq', '1cb7', '3a1m', '4zu7', '1acc', '1l5j', '6rib', '2jer', '1air', '2d40', '2fla', '1qte', '2kl8', '1dbv', '2obg', '7jvi', '2z0q', '1yox', '1f6w', '3i48', '3zds', '4puq', '1qre', '6e5c', '1cts', '1hin', '2qnz', '3ub3', '1idj', '3obw', '1dab', '3uxh', '4osd', '4aq6', '4aq2', '4fl6', '2ln3', '1znp']\n",
    "\n",
    "cd20s = ['6PE9', '6PE8', '1QSC', '6BRB', '3LKJ',\n",
    "         '6PE7', '1ALY']\n",
    "ms_related = ['6H24', '1PY9', '5HIU', '6FG1', '6FG2', '4Q6R', '4GMV']\n",
    "\n",
    "custom_dataset_pdbids = cd20s + ms_related + beta_helix_and_friends\n",
    "\n",
    "import pandas as pd\n",
    "print(f\"Custom dataset has {len(custom_dataset_pdbids)} samples\")\n",
    "print(pd.Series(custom_dataset_pdbids).unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 69 PDB IDs... 6pe9, 6pe8, 1qsc, 6brb, 3lkj, 6pe7, 1aly, 6h24, 1py9, 5hiu, 6fg1, 6fg2, 4q6r, 4gmv, 2jp7, 1prp, 3nxq, 1gca, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1pcl, 1xiq, 2pqe, 1kzq, 4mzu, 1wpc, 1fnu, 4g6r, 4jj2, 3hno, 1lxa, 6ria, 1hg9, 1dcq, 1cb7, 3a1m, 4zu7, 1acc, 1l5j, 6rib, 2jer, 1air, 2d40, 2fla, 1qte, 2kl8, 1dbv, 2obg, 7jvi, 2z0q, 1yox, 1f6w, 3i48, 3zds, 4puq, 1qre, 6e5c, 1cts, 1hin, 2qnz, 3ub3, 1idj, 3obw, 1dab, 3uxh, 4osd, 4aq6, 4aq2, 4fl6, 2ln3, 1znp, \n",
      "Done\n",
      "Number of fetched datums: 287, and 69 assemblies\n"
     ]
    }
   ],
   "source": [
    "class FetchPDBids:\n",
    "    \"\"\"Fetch PDB ids as AssemblyDatums.\"\"\"\n",
    "    def __init__(self, pdb_ids: List[str]):\n",
    "        self.pdb_ids = [pdb_id.lower() for pdb_id in pdb_ids]\n",
    "        self.datums = []\n",
    "        self.assemblies = []\n",
    "        self.transformed = []  # list of transformed ProteinDatums\n",
    "\n",
    "    def __call__(self):\n",
    "        print(f\"Fetching {len(self.pdb_ids)} PDB IDs...\", end=\" \")\n",
    "        for pdb_id in self.pdb_ids:\n",
    "            assembly = AssemblyDatum.fetch_pdb_id(pdb_id,)\n",
    "            self.assemblies.append(assembly)\n",
    "            for datum in assembly.protein_data:\n",
    "                # datum.idcode = pdb_id\n",
    "                self.datums.append(datum)\n",
    "            print(f\"{pdb_id}, \", end=\"\")\n",
    "        print(\"\\nDone\")\n",
    "\n",
    "    def togrid(self, k=None, num_columns=3, use_transformed=False):\n",
    "        if k is None:\n",
    "            k = len(self.datums)\n",
    "        if use_transformed:\n",
    "            if self.transformed == []:\n",
    "                self.transform()\n",
    "            datum_grid = self.make_grid(self.transformed[:k], num_columns)\n",
    "        else:\n",
    "            datum_grid = self.make_grid(self.datums[:k], num_columns)\n",
    "        return datum_grid\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_grid(datums: List[ProteinDatum], num_columns=3):\n",
    "        return [datums[i:i + num_columns] for i in range(0, len(datums), num_columns)]\n",
    "    \n",
    "    def transform(self):\n",
    "        self.transformed = [transform(datum) for datum in self.datums]\n",
    "\n",
    "fetcher = FetchPDBids(custom_dataset_pdbids)\n",
    "fetcher()\n",
    "fetcher.transform()\n",
    "print(f\"Number of fetched datums: {len(fetcher.datums)}, and {len(fetcher.assemblies)} assemblies\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "[ 6 12 22 15 19  8 18 17  6 18 13  3 22 18 13 10  9  4  3 19 12  5  7 14\n",
      " 18 18  8 18 13 13  5 13 10  5  8 14  5 21 13 19 20 16  8  8 14 17 10  8\n",
      " 17 17 14 13 13 12 21 20  3 18 19  4  9 18 10 22 17  6  4 16 18 10 18 10\n",
      " 18 10 19  6 16 19 13 19 12 18 18 13  8  3  9  6 22  3 22 21 21  7  8  5\n",
      "  6 21 19 21 17 13 19 16 10  8 10 19 14 13  9 12 14  4 19 22  3  3 17 18\n",
      " 22 16 12 16 17 17 18  6  9  8 13 14 18 10 19  3 18 22 22  7 13 13  5  5\n",
      " 16 21 17  4  9  3 14 22  8 20 14 22  6  5  3 13  8 18 10  5 18  8  9 18\n",
      " 22 19  9  8  6 18 14  6 18 19 21 18 13 18 18 19 13 19 13 18 14  3  6 21\n",
      "  9 14 11 14 22 21  3  7  9 22 19 11  8 10 13 18 18 17 22 19 14 18 16  5\n",
      "  4 10  9  7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ASP',\n",
       " 'ILE',\n",
       " 'VAL',\n",
       " 'MET',\n",
       " 'THR',\n",
       " 'GLN',\n",
       " 'SER',\n",
       " 'PRO',\n",
       " 'ASP',\n",
       " 'SER',\n",
       " 'LEU',\n",
       " 'ALA',\n",
       " 'VAL',\n",
       " 'SER',\n",
       " 'LEU',\n",
       " 'GLY',\n",
       " 'GLU',\n",
       " 'ARG',\n",
       " 'ALA',\n",
       " 'THR',\n",
       " 'ILE',\n",
       " 'ASN',\n",
       " 'CYS',\n",
       " 'LYS',\n",
       " 'SER',\n",
       " 'SER',\n",
       " 'GLN',\n",
       " 'SER',\n",
       " 'LEU',\n",
       " 'LEU',\n",
       " 'ASN',\n",
       " 'LEU',\n",
       " 'GLY',\n",
       " 'ASN',\n",
       " 'GLN',\n",
       " 'LYS',\n",
       " 'ASN',\n",
       " 'TYR',\n",
       " 'LEU',\n",
       " 'THR',\n",
       " 'TRP',\n",
       " 'PHE',\n",
       " 'GLN',\n",
       " 'GLN',\n",
       " 'LYS',\n",
       " 'PRO',\n",
       " 'GLY',\n",
       " 'GLN',\n",
       " 'PRO',\n",
       " 'PRO',\n",
       " 'LYS',\n",
       " 'LEU',\n",
       " 'LEU',\n",
       " 'ILE',\n",
       " 'TYR',\n",
       " 'TRP',\n",
       " 'ALA',\n",
       " 'SER',\n",
       " 'THR',\n",
       " 'ARG',\n",
       " 'GLU',\n",
       " 'SER',\n",
       " 'GLY',\n",
       " 'VAL',\n",
       " 'PRO',\n",
       " 'ASP',\n",
       " 'ARG',\n",
       " 'PHE',\n",
       " 'SER',\n",
       " 'GLY',\n",
       " 'SER',\n",
       " 'GLY',\n",
       " 'SER',\n",
       " 'GLY',\n",
       " 'THR',\n",
       " 'ASP',\n",
       " 'PHE',\n",
       " 'THR',\n",
       " 'LEU',\n",
       " 'THR',\n",
       " 'ILE',\n",
       " 'SER',\n",
       " 'SER',\n",
       " 'LEU',\n",
       " 'GLN',\n",
       " 'ALA',\n",
       " 'GLU',\n",
       " 'ASP',\n",
       " 'VAL',\n",
       " 'ALA',\n",
       " 'VAL',\n",
       " 'TYR',\n",
       " 'TYR',\n",
       " 'CYS',\n",
       " 'GLN',\n",
       " 'ASN',\n",
       " 'ASP',\n",
       " 'TYR',\n",
       " 'THR',\n",
       " 'TYR',\n",
       " 'PRO',\n",
       " 'LEU',\n",
       " 'THR',\n",
       " 'PHE',\n",
       " 'GLY',\n",
       " 'GLN',\n",
       " 'GLY',\n",
       " 'THR',\n",
       " 'LYS',\n",
       " 'LEU',\n",
       " 'GLU',\n",
       " 'ILE',\n",
       " 'LYS',\n",
       " 'ARG',\n",
       " 'THR',\n",
       " 'VAL',\n",
       " 'ALA',\n",
       " 'ALA',\n",
       " 'PRO',\n",
       " 'SER',\n",
       " 'VAL',\n",
       " 'PHE',\n",
       " 'ILE',\n",
       " 'PHE',\n",
       " 'PRO',\n",
       " 'PRO',\n",
       " 'SER',\n",
       " 'ASP',\n",
       " 'GLU',\n",
       " 'GLN',\n",
       " 'LEU',\n",
       " 'LYS',\n",
       " 'SER',\n",
       " 'GLY',\n",
       " 'THR',\n",
       " 'ALA',\n",
       " 'SER',\n",
       " 'VAL',\n",
       " 'VAL',\n",
       " 'CYS',\n",
       " 'LEU',\n",
       " 'LEU',\n",
       " 'ASN',\n",
       " 'ASN',\n",
       " 'PHE',\n",
       " 'TYR',\n",
       " 'PRO',\n",
       " 'ARG',\n",
       " 'GLU',\n",
       " 'ALA',\n",
       " 'LYS',\n",
       " 'VAL',\n",
       " 'GLN',\n",
       " 'TRP',\n",
       " 'LYS',\n",
       " 'VAL',\n",
       " 'ASP',\n",
       " 'ASN',\n",
       " 'ALA',\n",
       " 'LEU',\n",
       " 'GLN',\n",
       " 'SER',\n",
       " 'GLY',\n",
       " 'ASN',\n",
       " 'SER',\n",
       " 'GLN',\n",
       " 'GLU',\n",
       " 'SER',\n",
       " 'VAL',\n",
       " 'THR',\n",
       " 'GLU',\n",
       " 'GLN',\n",
       " 'ASP',\n",
       " 'SER',\n",
       " 'LYS',\n",
       " 'ASP',\n",
       " 'SER',\n",
       " 'THR',\n",
       " 'TYR',\n",
       " 'SER',\n",
       " 'LEU',\n",
       " 'SER',\n",
       " 'SER',\n",
       " 'THR',\n",
       " 'LEU',\n",
       " 'THR',\n",
       " 'LEU',\n",
       " 'SER',\n",
       " 'LYS',\n",
       " 'ALA',\n",
       " 'ASP',\n",
       " 'TYR',\n",
       " 'GLU',\n",
       " 'LYS',\n",
       " 'HIS',\n",
       " 'LYS',\n",
       " 'VAL',\n",
       " 'TYR',\n",
       " 'ALA',\n",
       " 'CYS',\n",
       " 'GLU',\n",
       " 'VAL',\n",
       " 'THR',\n",
       " 'HIS',\n",
       " 'GLN',\n",
       " 'GLY',\n",
       " 'LEU',\n",
       " 'SER',\n",
       " 'SER',\n",
       " 'PRO',\n",
       " 'VAL',\n",
       " 'THR',\n",
       " 'LYS',\n",
       " 'SER',\n",
       " 'PHE',\n",
       " 'ASN',\n",
       " 'ARG',\n",
       " 'GLY',\n",
       " 'GLU',\n",
       " 'CYS']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.utils import residue_map\n",
    "\n",
    "print(fetcher.datums[5].residue_token.dtype)\n",
    "residue_map(fetcher.datums[5].residue_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_assemblies(assemblies: List[AssemblyDatum], start_level: int = 0):\n",
    "    \"\"\"Encode a list of assemblies through ophiuchus.\"\"\"\n",
    "\n",
    "    encoded_dataset = defaultdict(dict)\n",
    "    sliced_dataset = defaultdict(partial(defaultdict, list))\n",
    "\n",
    "    for idx, assembly in tqdm(enumerate(assemblies)):\n",
    "        \n",
    "        for protein_index in range(len(assembly.protein_data)):\n",
    "\n",
    "            datum = assembly.protein_data[protein_index]\n",
    "            datum_input = transform(datum)\n",
    "            datum_input.idcode = None\n",
    "            output = autoencoder(datum_input)\n",
    "\n",
    "            for level in range(start_level, len(output.encoder_internals)):\n",
    "                tc = output.encoder_internals[level]\n",
    "                mask = tc.mask_coord[0]\n",
    "                tc = np.array(tc.irreps_array.filter('0e').array)[0][mask]\n",
    "                encoded_dataset[datum.idcode][level] = tc\n",
    "\n",
    "                slices = slices_per_level[level]\n",
    "                for i, (start, end) in enumerate(slices):\n",
    "                    if start <= len(datum):\n",
    "                        sliced_dataset[datum.idcode][level].append(datum[start:end])\n",
    "        # if idx > 5:\n",
    "        #     break\n",
    "    return encoded_dataset, sliced_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:03,  4.89it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [00:19,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.8 s, sys: 3.08 s, total: 23.9 s\n",
      "Wall time: 19.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This takes about 3 mins (on ~55 assemblies)...most of it due to initial compilation\n",
    "encoded_dataset, sliced_dataset = encode_assemblies(fetcher.assemblies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "path = '../data'\n",
    "\n",
    "with open(f'{path}/encoded_dataset_custom.pkl', 'wb') as f:\n",
    "    pickle.dump(encoded_dataset, f)\n",
    "\n",
    "# Save the sliced dataset\n",
    "with open(f'{path}/sliced_dataset_custom.pkl', 'wb') as f:\n",
    "    pickle.dump(sliced_dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moleculib.graphics.py3Dmol import plot_py3dmol_grid\n",
    "plot_py3dmol_grid(fetcher.togrid(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE that it does not really make sense to compute tsne on these embeddings since they are not a part of the full dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tsne(encoded_dataset, n_layers, start_level=0):\n",
    "    \"\"\"Using the encoded dataset, get tsne coords and colors for each level.\"\"\"\n",
    "    encoded_dataset_tsne = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n",
    "    for level in range(start_level, n_layers):\n",
    "        level_data = []\n",
    "        for key, value in list(encoded_dataset.items()):\n",
    "            level_data.append(value[level])\n",
    "        level_data = np.concatenate(level_data)\n",
    "        \n",
    "        print(f'computing position tsne for level {level}: {level_data.shape}')\n",
    "        position = TSNE(n_components=2, perplexity=3, learning_rate='auto', init='random').fit_transform(level_data)\n",
    "        print(f'computing color tsne for level {level}: {level_data.shape}')\n",
    "        colors = TSNE(n_components=3, perplexity=3, learning_rate='auto', init='random').fit_transform(level_data)\n",
    "        colors = (colors - colors.min())\n",
    "        colors = (colors * 255 / colors.max()).astype(np.int32)\n",
    "        colors = [f'rgb({r}, {g}, {b})' for r, g, b in colors]\n",
    "\n",
    "        cumsum = 0        \n",
    "        for _, key in enumerate(list(encoded_dataset.keys())):\n",
    "            len_ = len(encoded_dataset[key][level])\n",
    "            encoded_dataset_tsne[key][level]['pos'] = position[cumsum:cumsum+len_].tolist()\n",
    "            encoded_dataset_tsne[key][level]['colors'] = colors[cumsum:cumsum+len_]\n",
    "            cumsum += len_\n",
    "    return encoded_dataset_tsne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing position tsne for level 0: (40648, 24)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoded_dataset_tsne \u001b[38;5;241m=\u001b[39m \u001b[43mmake_tsne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m, in \u001b[0;36mmake_tsne\u001b[0;34m(encoded_dataset, n_layers, start_level)\u001b[0m\n\u001b[1;32m      8\u001b[0m level_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(level_data)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputing position tsne for level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m position \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperplexity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputing color tsne for level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m colors \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(level_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1119\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[0;32m-> 1119\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1012\u001b[0m, in \u001b[0;36mTSNE._fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# Degrees of freedom of the Student's t-distribution. The suggestion\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# degrees_of_freedom = n_components - 1 comes from\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# \"Learning a Parametric Embedding by Preserving Local Structure\"\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# Laurens van der Maaten, 2009.\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m degrees_of_freedom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tsne\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneighbors_nn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_num_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_num_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1080\u001b[0m, in \u001b[0;36mTSNE._tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1078\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmomentum\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m\n\u001b[1;32m   1079\u001b[0m     opt_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter_without_progress\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_without_progress\n\u001b[0;32m-> 1080\u001b[0m     params, kl_divergence, it \u001b[38;5;241m=\u001b[39m \u001b[43m_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopt_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# Save the final number of iterations\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m it\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:399\u001b[0m, in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# only compute the error when needed\u001b[39;00m\n\u001b[1;32m    397\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_error\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m check_convergence \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m n_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 399\u001b[0m error, grad \u001b[38;5;241m=\u001b[39m \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m inc \u001b[38;5;241m=\u001b[39m update \u001b[38;5;241m*\u001b[39m grad \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    402\u001b[0m dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minvert(inc)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:280\u001b[0m, in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    277\u001b[0m indptr \u001b[38;5;241m=\u001b[39m P\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    279\u001b[0m grad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(X_embedded\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 280\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43m_barnes_hut_tsne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_P\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdegrees_of_freedom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m*\u001b[39m (degrees_of_freedom \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m/\u001b[39m degrees_of_freedom\n\u001b[1;32m    294\u001b[0m grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mravel()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoded_dataset_tsne = make_tsne(encoded_dataset, n_layers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'{path}/encoded_dataset_tsne.json', 'w') as f:\n",
    "    json.dump(encoded_dataset_tsne, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
