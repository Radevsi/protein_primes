{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Dataset\n",
    "\n",
    "Make an analysis dataset with both encoded representations and custom requests.\n",
    "\n",
    "This file takes embeddings already produced by the ophiuchus model, processes them (removing Nones and the amino acid embedding level). It also combines any other encodings generated from other files to the other embeddings, producing a single, coherent dataset with which to compare in a separate notebook.\n",
    "\n",
    "Use case is when the standard dataset used to generate mass embeddings by ophiuchus does not contain particular pairs that are of interest to us. We can then use this notebook to combine multiple embedded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once cell\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from typing import List, Dict, Tuple, Union \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from moleculib.protein.datum import ProteinDatum\n",
    "from moleculib.graphics.py3Dmol import plot_py3dmol, plot_py3dmol_grid\n",
    "\n",
    "\n",
    "from helpers.database import populate_representations, whatis\n",
    "from helpers.edges import connect_edges, CascadingEdges\n",
    "from helpers.data_processing import LoadData, save_df, save_edges\n",
    "\n",
    "from helpers.utils import CheckPDBs, aa_map, residue_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## General useful functions\n",
    "\n",
    "def load_data(folder, embeddings_file, sliced_proteins_file, tsne_file):\n",
    "\n",
    "    # Load data from files\n",
    "    dataloader = LoadData(folder, embeddings_file, sliced_proteins_file, tsne_file)\n",
    "    dataloader.load_all()\n",
    "\n",
    "    # Make objects\n",
    "    if tsne_file is not None:\n",
    "        reps, _ = populate_representations(dataloader.encoded_dataset, \n",
    "                                        dataloader.sliced_dataset, \n",
    "                                        dataloader.tsne_dataset)\n",
    "    else:\n",
    "        reps, _ = populate_representations(dataloader.encoded_dataset, dataloader.sliced_dataset)\n",
    "        \n",
    "    df = reps.to_dataframe()\n",
    "    print(f\"Loaded full dataset: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def get_info(df, verbose=True):\n",
    "    \"\"\"Return basic DataFrame information.\"\"\"\n",
    "    # Calculate the number of nodes per level\n",
    "    nodes_per_level = df.groupby('level').size()\n",
    "\n",
    "    # Count the \"None\" datums\n",
    "    n_none_datums = df[df['datum'].isnull()].shape[0]\n",
    "    if verbose:\n",
    "        print(\"Shape of nodes per level: {}\".format(nodes_per_level.shape))\n",
    "        display(nodes_per_level)\n",
    "        print(f\"Number of None datums: {n_none_datums}\")\n",
    "\n",
    "def process_data(df):\n",
    "    \"\"\"Perform basic data processing. Drop Nones, and drop amino acid-level\n",
    "        embeddings...\n",
    "    \"\"\"\n",
    "    df_sample = df.dropna(subset=['datum']).reset_index(drop=True)  # drop nans\n",
    "    df_sample = df_sample[df_sample['level'] != 0].reset_index(drop=True)  # drop level 0\n",
    "    print(f\"Shape of sample after drops: {df_sample.shape}\")\n",
    "    return df_sample\n",
    "\n",
    "\n",
    "def make_edges(df, kernel, stride): \n",
    "    \"\"\"Make edges and cascades.\"\"\"\n",
    "    # Compute edges\n",
    "    edges_top_down, edges_bottom_up, n_misses = connect_edges(df, kernel, stride)\n",
    "    make_cascades = CascadingEdges(edges_bottom_up)\n",
    "    print(f\"Misses: {n_misses}\")\n",
    "\n",
    "    return edges_top_down, edges_bottom_up, make_cascades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Embeddings\n",
    "\n",
    "Load the embeddings and sliced datums of standard ophiuchus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from data/denim-energy-1008-embeddings/encoded_dataset.pkl\n",
      "Loading sliced proteins from data/denim-energy-1008-embeddings/sliced_dataset.pkl\n",
      "Loaded full dataset: (431465, 7)\n",
      "Shape of nodes per level: (5,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "level\n",
       "0    221142\n",
       "1    111222\n",
       "2     56215\n",
       "3     28602\n",
       "4     14284\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of None datums: 5375\n",
      "Shape of sample after drops: (204948, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load the standard dataset\n",
    "# Since we will do TSNE together later, don't load it now.\n",
    "\n",
    "FOLDER = \"data/denim-energy-1008-embeddings\"\n",
    "embeddings_file = \"encoded_dataset.pkl\"\n",
    "sliced_proteins_file = \"sliced_dataset.pkl\"\n",
    "tsne_file = \"encoded_dataset_tsne.json\"\n",
    "\n",
    "df = load_data(FOLDER, embeddings_file=embeddings_file, \n",
    "               sliced_proteins_file=sliced_proteins_file, \n",
    "               tsne_file=None)\n",
    "get_info(df)\n",
    "df_sample = process_data(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get other data sources of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from data/custom-embeddings/encoded_dataset_custom.pkl\n",
      "Loading sliced proteins from data/custom-embeddings/sliced_dataset_custom.pkl\n",
      "Loaded full dataset: (99206, 7)\n",
      "Shape of sample after drops: (47114, 7)\n"
     ]
    }
   ],
   "source": [
    "customloader = LoadData(FOLDER=\"data/custom-embeddings\",\n",
    "                        embeddings_file=\"encoded_dataset_custom.pkl\",\n",
    "                        sliced_proteins_file=\"sliced_dataset_custom.pkl\")\n",
    "customloader.load_all()\n",
    "\n",
    "# Make objects\n",
    "custom_reps, _ = populate_representations(customloader.encoded_dataset, customloader.sliced_dataset)\n",
    "custom_df = custom_reps.to_dataframe()\n",
    "print(f\"Loaded full dataset: {custom_df.shape}\")\n",
    "\n",
    "# Process as before\n",
    "custom_sample = process_data(custom_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Shape of new_df: (252062, 7)\n"
     ]
    }
   ],
   "source": [
    "# Append the rows of custom_sample to df_sample\n",
    "new_df = pd.concat([df_sample, custom_sample], ignore_index=True)\n",
    "\n",
    "# Check that the indices are unique and increasing\n",
    "print(new_df.index.is_monotonic_increasing)\n",
    "print(new_df.index.is_unique)\n",
    "print(f\"Shape of new_df: {new_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pdb_id             0\n",
       "level              0\n",
       "level_idx          0\n",
       "scalar_rep         0\n",
       "datum              0\n",
       "pos           252062\n",
       "color         252062\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misses: 11933\n",
      "Object 0: ({81411: [81286, 81287, 81288, 81289, 81290], 81412...) is a dictionary with length 116399\n",
      "Object 1: ({81286: 81411, 81287: 81411, 81288: 81412, 81289: ...) is a dictionary with length 236217\n"
     ]
    }
   ],
   "source": [
    "# Now do edges for the new dataframe\n",
    "\n",
    "edges_top_down, edges_bottom_up, make_cascades = make_edges(new_df, kernel=5, stride=2)\n",
    "whatis(edges_top_down, edges_bottom_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as JSON to data//encoded_dataset_combined.json\n",
      "Saved new_df\n",
      "edges_bottom_up has been saved to data//edges_bottom_up.json\n"
     ]
    }
   ],
   "source": [
    "# Now save the new dataframe and edges. This is what will be used for the rest of the analysis.\n",
    "save_df(new_df, \"encoded_dataset_combined\")\n",
    "print(\"Saved new_df\")\n",
    "save_edges(edges_bottom_up, \"edges_bottom_up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as CSV to data/combined_dataset.csv\n",
      "edges_bottom_up dictionary saved as pickle to data/edges_bottom_up.pkl\n",
      "CPU times: user 1min 28s, sys: 797 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "new_df.to_csv(\"data/master_dataframe.csv\")\n",
    "print(\"DataFrame saved as CSV to data/combined_dataset.csv\")\n",
    "\n",
    "# Save the edges_bottom_up dictionary as a pickle file\n",
    "import pickle\n",
    "with open(\"data/master_edges.pkl\", \"wb\") as f:\n",
    "    pickle.dump(edges_bottom_up, f)\n",
    "print(\"edges_bottom_up dictionary saved as pickle to data/edges_bottom_up.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as pickle to data/master_dataframe.pkl\n",
      "CPU times: user 13.6 s, sys: 2.31 s, total: 15.9 s\n",
      "Wall time: 16.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Save the DataFrame as a pickle file\n",
    "with open(\"data/master_dataframe.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_df, f)\n",
    "print(\"DataFrame saved as pickle to data/master_dataframe.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TSNE Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataframe for TSNE processing\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def compute_tsne(df: pd.DataFrame):\n",
    "    \"\"\"Given a dataframe (corresponding to our database), compute tsne coords and colors.\"\"\"\n",
    "    # encoded_dataset_tsne = defaultdict(lambda : defaultdict(lambda : defaultdict(dict)))\n",
    "\n",
    "    # Get unique levels\n",
    "    positions_lst = []\n",
    "    colors_lst = []\n",
    "\n",
    "    levels = sorted(df['level'].unique())\n",
    "    for level in levels:\n",
    "        df_for_level = df[df['level'] == level]\n",
    "        level_data = np.array(df_for_level['scalar_rep'].values.tolist())\n",
    "\n",
    "        print(f'computing position tsne for level {level}: {level_data.shape}')\n",
    "        position = TSNE(n_components=2, perplexity=3, learning_rate='auto', init='random').fit_transform(level_data)\n",
    "        print(f'computing color tsne for level {level}: {level_data.shape}')\n",
    "        colors = TSNE(n_components=3, perplexity=3, learning_rate='auto', init='random').fit_transform(level_data)\n",
    "        colors = (colors - colors.min())\n",
    "        colors = (colors * 255 / colors.max()).astype(np.int32)\n",
    "        colors = [f'rgb({r}, {g}, {b})' for r, g, b in colors]\n",
    "\n",
    "        # positions[level] = position\n",
    "        # colors[level] = colors\n",
    "\n",
    "        positions_lst.extend(position.tolist())\n",
    "        colors_lst.extend(colors)\n",
    "\n",
    "    return positions_lst, colors_lst\n",
    "\n",
    "    #     cumsum = 0\n",
    "    #     pdb_groups = df_for_level.groupby('pdb_id')\n",
    "    #     for pdb, group in pdb_groups:\n",
    "    #         len_ = group.shape[0]\n",
    "    #         encoded_dataset_tsne[pdb][level]['pos'] = position[cumsum:cumsum+len_].tolist()\n",
    "    #         encoded_dataset_tsne[pdb][level]['colors'] = colors[cumsum:cumsum+len_]\n",
    "    #         cumsum += len_\n",
    "\n",
    "    # return encoded_dataset_tsne\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 7)\n",
      "computing position tsne for level 1: (672, 33)\n",
      "computing color tsne for level 1: (672, 33)\n",
      "computing position tsne for level 2: (358, 46)\n",
      "computing color tsne for level 2: (358, 46)\n",
      "computing position tsne for level 3: (165, 64)\n",
      "computing color tsne for level 3: (165, 64)\n",
      "computing position tsne for level 4: (65, 89)\n",
      "computing color tsne for level 4: (65, 89)\n"
     ]
    }
   ],
   "source": [
    "# Get a 10% random sample of the dataframe\n",
    "small_df = new_df.sample(frac=0.005, random_state=42)\n",
    "print(small_df.shape)\n",
    "positions, colors = compute_tsne(small_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>level</th>\n",
       "      <th>level_idx</th>\n",
       "      <th>scalar_rep</th>\n",
       "      <th>datum</th>\n",
       "      <th>pos</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97653</th>\n",
       "      <td>1d3bD</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>[-0.13821925, 0.24543715, -0.33401486, 0.01873...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[-35.692317962646484, 32.21705627441406]</td>\n",
       "      <td>rgb(84, 108, 228)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73996</th>\n",
       "      <td>1bvyF</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>[-0.08123147, 0.25039104, -0.2238249, 0.043804...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[12.129412651062012, 0.21517214179039001]</td>\n",
       "      <td>rgb(165, 112, 189)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177959</th>\n",
       "      <td>1eovA</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>[-0.1622783, 0.27444598, -0.31894395, 0.078085...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[-0.19315895438194275, 16.106788635253906]</td>\n",
       "      <td>rgb(150, 79, 201)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58740</th>\n",
       "      <td>1ez0A</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>[0.03047585, 0.23298316, -0.13053995, 0.007102...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[46.32844161987305, -10.210537910461426]</td>\n",
       "      <td>rgb(195, 139, 114)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154858</th>\n",
       "      <td>1axtL</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>[-0.46172005, -0.21491313, -0.6202147, 0.16822...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[38.54891586303711, -34.35532760620117]</td>\n",
       "      <td>rgb(235, 126, 160)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pdb_id  ...               color\n",
       "97653   1d3bD  ...   rgb(84, 108, 228)\n",
       "73996   1bvyF  ...  rgb(165, 112, 189)\n",
       "177959  1eovA  ...   rgb(150, 79, 201)\n",
       "58740   1ez0A  ...  rgb(195, 139, 114)\n",
       "154858  1axtL  ...  rgb(235, 126, 160)\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df['pos'] = positions\n",
    "small_df['color'] = colors\n",
    "print(small_df.shape)\n",
    "small_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_copied = new_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252062, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try the full thing\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing position tsne for level 1: (135663, 33)\n",
      "computing color tsne for level 1: (135663, 33)\n",
      "computing position tsne for level 2: (67370, 46)\n",
      "computing color tsne for level 2: (67370, 46)\n",
      "computing position tsne for level 3: (33193, 64)\n",
      "computing color tsne for level 3: (33193, 64)\n",
      "computing position tsne for level 4: (15836, 89)\n",
      "computing color tsne for level 4: (15836, 89)\n",
      "CPU times: user 7h 12min 9s, sys: 8min 15s, total: 7h 20min 24s\n",
      "Wall time: 56min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full_positions, full_colors = compute_tsne(new_df)\n",
    "\n",
    "# This cell took 56 minutes to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the full positions and full colors to pickle files\n",
    "# Since they are Python lists, we need to use pickle directly\n",
    "import pickle\n",
    "\n",
    "with open('full_positions.pkl', 'wb') as f:\n",
    "    pickle.dump(full_positions, f)\n",
    "\n",
    "with open('full_colors.pkl', 'wb') as f:\n",
    "    pickle.dump(full_colors, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(252062, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pdb_id</th>\n",
       "      <th>level</th>\n",
       "      <th>level_idx</th>\n",
       "      <th>scalar_rep</th>\n",
       "      <th>datum</th>\n",
       "      <th>pos</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1f00I</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.081032045, 0.62376326, 0.28515857, 0.197421...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[-8.188774108886719, 106.9360122680664]</td>\n",
       "      <td>rgb(117, 49, 59)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1f00I</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.28887343, 0.001341799, -0.54696304, 0.1838...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[-80.61167907714844, 33.45246887207031]</td>\n",
       "      <td>rgb(141, 117, 56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1f00I</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.11274243, 0.2764013, -0.36209202, 0.011574...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[71.27960205078125, 38.42264175415039]</td>\n",
       "      <td>rgb(92, 128, 174)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1f00I</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.12116315, 0.50699997, -0.15239324, 0.09882...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[45.77057647705078, -70.40210723876953]</td>\n",
       "      <td>rgb(91, 76, 112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1f00I</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.14587262, 0.10403667, -0.38717338, 0.06709...</td>\n",
       "      <td>(((&lt;moleculib.protein.datum.ProteinDatum objec...</td>\n",
       "      <td>[38.595611572265625, 48.74850845336914]</td>\n",
       "      <td>rgb(93, 210, 190)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pdb_id  level  ...                                      pos              color\n",
       "0  1f00I      1  ...  [-8.188774108886719, 106.9360122680664]   rgb(117, 49, 59)\n",
       "1  1f00I      1  ...  [-80.61167907714844, 33.45246887207031]  rgb(141, 117, 56)\n",
       "2  1f00I      1  ...   [71.27960205078125, 38.42264175415039]  rgb(92, 128, 174)\n",
       "3  1f00I      1  ...  [45.77057647705078, -70.40210723876953]   rgb(91, 76, 112)\n",
       "4  1f00I      1  ...  [38.595611572265625, 48.74850845336914]  rgb(93, 210, 190)\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Append\n",
    "\n",
    "new_df['pos'] = full_positions\n",
    "new_df['color'] = full_colors\n",
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of None datums in new_df: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for None datums in the new_df DataFrame\n",
    "none_datums_count = new_df['datum'].isnull().sum()\n",
    "print(f\"Number of None datums in new_df: {none_datums_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misses: 11933\n"
     ]
    }
   ],
   "source": [
    "# Now we compute edges on this \n",
    "edges_top_down, edges_bottom_up, make_cascades = make_edges(new_df, kernel=5, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 0: ({81286: 81411, 81287: 81411, 81288: 81412, 81289: ...) is a dictionary with length 236217\n"
     ]
    }
   ],
   "source": [
    "whatis(edges_bottom_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a problem whereby some datums have length 0. At this point we should just filter those out. It should be only 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Drop rows where the length of the datum object is 0\n",
    "final_df = df[df['datum'].apply(len) > 0]\n",
    "print(final_df.shape)\n",
    "\n",
    "edges_top_down, edges_bottom_up, n_misses = connect_edges(final_df, 5, 2)\n",
    "\n",
    "# from helpers.data_processing import save_df, save_edges\n",
    "\n",
    "# Save like this\n",
    "if False:\n",
    "    with open(\"data/master_dataframe.pkl\", \"wb\") as f:\n",
    "        pickle.dump(final_df, f)\n",
    "    print(\"final_df saved as pickle.\")\n",
    "\n",
    "    with open(\"data/master_edges.pkl\", \"wb\") as f:\n",
    "        pickle.dump(edges_bottom_up, f)\n",
    "    print(\"edges_bottom_up saved as pickle.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(new_df, \"data/master_dataframe.pkl\")\n",
    "save_edges(edges_bottom_up, \"data/master_edges.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251038, 7) (251038, 5)\n",
      "DataFrame saved as JSON to /Users/moniradev/Documents/projects/foreign/picture-picture/src/data/master_dataframe.json\n",
      "edges_bottom_up has been saved to /Users/moniradev/Documents/projects/foreign/picture-picture/src/data/master_edges.json\n"
     ]
    }
   ],
   "source": [
    "from helpers.data_processing import save_edges, save_df\n",
    "\n",
    "# One final processing scheme to make the data usable in react.\n",
    "\n",
    "path_to_visual = \"/Users/moniradev/Documents/projects/foreign/picture-picture/src/data\"\n",
    "df_for_visual = df.drop(columns=['scalar_rep', 'datum'])\n",
    "print(df.shape, df_for_visual.shape)\n",
    "df_for_visual.tail()\n",
    "\n",
    "save_df(df_for_visual, filename=\"master_dataframe\", folder=path_to_visual)\n",
    "save_edges(edges, filename=\"master_edges\", folder=path_to_visual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique pdbs: 287\n",
      "Number of unique assemblies: 66\n",
      "Shape of nodes per level: (5,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "level\n",
       "0    51090\n",
       "1    25599\n",
       "2    12849\n",
       "3     6484\n",
       "4     3184\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of None datums: 1002\n"
     ]
    }
   ],
   "source": [
    "custom_pdbs = custom_df['pdb_id'].unique()\n",
    "print(f\"Number of unique pdbs: {len(custom_pdbs)}\")\n",
    "\n",
    "custom_assemblies = pd.Series([pdb[:-1] for pdb in custom_pdbs]).unique()\n",
    "print(f\"Number of unique assemblies: {len(custom_assemblies)}\")\n",
    "\n",
    "get_info(custom_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
